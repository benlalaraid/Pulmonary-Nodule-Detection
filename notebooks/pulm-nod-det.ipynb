{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10807996,"sourceType":"datasetVersion","datasetId":6708959}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:26:03.947580Z","iopub.execute_input":"2025-03-11T13:26:03.947973Z","iopub.status.idle":"2025-03-11T13:26:10.398440Z","shell.execute_reply.started":"2025-03-11T13:26:03.947940Z","shell.execute_reply":"2025-03-11T13:26:10.397254Z"}},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.87-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.87-py3-none-any.whl (923 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m923.8/923.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: ultralytics-thop, ultralytics\nSuccessfully installed ultralytics-8.3.87 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport xml.etree.ElementTree as ET\nimport random\nimport shutil\nimport numpy as np\nimport cv2\nimport torch\nfrom ultralytics import YOLO\nfrom sklearn.metrics import f1_score\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:26:10.400295Z","iopub.execute_input":"2025-03-11T13:26:10.400721Z","iopub.status.idle":"2025-03-11T13:26:16.112140Z","shell.execute_reply.started":"2025-03-11T13:26:10.400681Z","shell.execute_reply":"2025-03-11T13:26:16.111353Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Define class names\nclasses = [\"nodule\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:26:16.113560Z","iopub.execute_input":"2025-03-11T13:26:16.114039Z","iopub.status.idle":"2025-03-11T13:26:16.117796Z","shell.execute_reply.started":"2025-03-11T13:26:16.114013Z","shell.execute_reply":"2025-03-11T13:26:16.116890Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============== DATA PROCESSING FUNCTIONS ==============\n\n# Convert XML annotations to YOLO format\ndef convert_annotation(xml_file, output_dir):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    image_width = int(root.find(\"size/width\").text)\n    image_height = int(root.find(\"size/height\").text)\n    \n    label_file = os.path.join(output_dir, os.path.basename(xml_file).replace(\".xml\", \".txt\"))\n    with open(label_file, \"w\") as f:\n        for obj in root.findall(\"object\"):\n            class_name = obj.find(\"name\").text\n            if class_name not in classes:\n                continue\n            class_id = classes.index(class_name)\n            \n            bbox = obj.find(\"bndbox\")\n            xmin = round(float(bbox.find(\"xmin\").text))\n            ymin = round(float(bbox.find(\"ymin\").text))\n            xmax = round(float(bbox.find(\"xmax\").text))\n            ymax = round(float(bbox.find(\"ymax\").text))\n            \n            # Convert to YOLO format\n            x_center = (xmin + xmax) / 2 / image_width\n            y_center = (ymin + ymax) / 2 / image_height\n            bbox_width = (xmax - xmin) / image_width\n            bbox_height = (ymax - ymin) / image_height\n            \n            f.write(f\"{class_id} {x_center} {y_center} {bbox_width} {bbox_height}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:26:16.118948Z","iopub.execute_input":"2025-03-11T13:26:16.119307Z","iopub.status.idle":"2025-03-11T13:26:16.136198Z","shell.execute_reply.started":"2025-03-11T13:26:16.119275Z","shell.execute_reply":"2025-03-11T13:26:16.135347Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Split dataset into train, validation, and test sets\ndef split_dataset(images_path, labels_path, train_ratio=0.7, val_ratio=0.15):\n    images = [f for f in os.listdir(images_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n    random.seed(42)  # For reproducibility\n    random.shuffle(images)\n    \n    train_size = int(len(images) * train_ratio)\n    val_size = int(len(images) * val_ratio)\n\n    train_files = images[:train_size]\n    val_files = images[train_size:train_size + val_size]\n    test_files = images[train_size + val_size:]\n\n    for folder in [\"train\", \"val\", \"test\"]:\n        os.makedirs(f\"dataset/images/{folder}\", exist_ok=True)\n        os.makedirs(f\"dataset/labels/{folder}\", exist_ok=True)\n    \n    # Copy images and corresponding labels to respective folders\n    for file_list, subset in zip([train_files, val_files, test_files], [\"train\", \"val\", \"test\"]):\n        for img_file in file_list:\n            # Copy image\n            src_img = os.path.join(images_path, img_file)\n            dst_img = os.path.join(f\"dataset/images/{subset}\", img_file)\n            shutil.copy(src_img, dst_img)\n            \n            # Copy label if exists\n            label_file = os.path.splitext(img_file)[0] + \".txt\"\n            src_label = os.path.join(labels_path, label_file)\n            if os.path.exists(src_label):\n                dst_label = os.path.join(f\"dataset/labels/{subset}\", label_file)\n                shutil.copy(src_label, dst_label)\n    \n    print(f\"Dataset split complete: {len(train_files)} training, {len(val_files)} validation, {len(test_files)} test images\")\n    return len(train_files), len(val_files), len(test_files)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:26:16.137112Z","iopub.execute_input":"2025-03-11T13:26:16.137419Z","iopub.status.idle":"2025-03-11T13:26:16.155285Z","shell.execute_reply.started":"2025-03-11T13:26:16.137384Z","shell.execute_reply":"2025-03-11T13:26:16.154630Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ============== DATA AUGMENTATION FUNCTIONS ==============\n\ndef apply_augmentations(image_dir, label_dir, output_image_dir, output_label_dir, num_augmentations=3):\n    \"\"\"\n    Apply augmentations to images and corresponding labels\n    \"\"\"\n    import albumentations as A\n    from albumentations.pytorch import ToTensorV2\n    \n    # Create output directories if they don't exist\n    os.makedirs(output_image_dir, exist_ok=True)\n    os.makedirs(output_label_dir, exist_ok=True)\n    \n    # Define augmentation pipeline\n    transform = A.Compose([\n        A.OneOf([\n            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n            A.RandomGamma(gamma_limit=(80, 120), p=0.5),\n            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.5),\n        ], p=0.5),\n        A.OneOf([\n            A.Blur(blur_limit=3, p=0.5),\n            A.MedianBlur(blur_limit=3, p=0.5),\n            A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n        ], p=0.5),\n        A.OneOf([\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n            A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n        ], p=0.5),\n        A.Flip(p=0.5),\n        A.RandomRotate90(p=0.5),\n    ], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n    \n    # Get list of images\n    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n    \n    augmented_count = 0\n    for img_file in image_files:\n        # Load image\n        img_path = os.path.join(image_dir, img_file)\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Load corresponding label if exists\n        label_path = os.path.join(label_dir, os.path.splitext(img_file)[0] + '.txt')\n        if not os.path.exists(label_path):\n            continue\n            \n        # Parse YOLO format labels\n        bboxes = []\n        class_labels = []\n        \n        with open(label_path, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                class_id = int(parts[0])\n                x_center, y_center, width, height = map(float, parts[1:5])\n                bboxes.append([x_center, y_center, width, height])\n                class_labels.append(class_id)\n        \n        # Skip if no bounding boxes\n        if not bboxes:\n            continue\n            \n        # Copy original image and label\n        shutil.copy(img_path, os.path.join(output_image_dir, img_file))\n        shutil.copy(label_path, os.path.join(output_label_dir, os.path.splitext(img_file)[0] + '.txt'))\n        \n        # Apply augmentations\n        for i in range(num_augmentations):\n            augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n            aug_image = augmented['image']\n            aug_bboxes = augmented['bboxes']\n            aug_class_labels = augmented['class_labels']\n            \n            # Skip if no bounding boxes after augmentation\n            if not aug_bboxes:\n                continue\n                \n            # Save augmented image\n            aug_img_file = f\"{os.path.splitext(img_file)[0]}_aug{i+1}{os.path.splitext(img_file)[1]}\"\n            aug_img_path = os.path.join(output_image_dir, aug_img_file)\n            cv2.imwrite(aug_img_path, cv2.cvtColor(aug_image, cv2.COLOR_RGB2BGR))\n            \n            # Save augmented labels\n            aug_label_file = f\"{os.path.splitext(img_file)[0]}_aug{i+1}.txt\"\n            aug_label_path = os.path.join(output_label_dir, aug_label_file)\n            \n            with open(aug_label_path, 'w') as f:\n                for bbox, class_id in zip(aug_bboxes, aug_class_labels):\n                    f.write(f\"{class_id} {bbox[0]} {bbox[1]} {bbox[2]} {bbox[3]}\\n\")\n            \n            augmented_count += 1\n    \n    print(f\"Created {augmented_count} augmented images\")\n    return augmented_count\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:26:16.156249Z","iopub.execute_input":"2025-03-11T13:26:16.156578Z","iopub.status.idle":"2025-03-11T13:26:16.176832Z","shell.execute_reply.started":"2025-03-11T13:26:16.156547Z","shell.execute_reply":"2025-03-11T13:26:16.176030Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ============== DATASET PREPARATION ==============\n\ndef prepare_dataset(annotations_path, images_path, apply_augmentation=True):\n    # Create base directories\n    os.makedirs(\"dataset/labels\", exist_ok=True)\n    \n    # Convert all annotations to YOLO format\n    print(\"Converting annotations to YOLO format...\")\n    for xml_file in os.listdir(annotations_path):\n        if xml_file.endswith(\".xml\"):\n            convert_annotation(os.path.join(annotations_path, xml_file), \"dataset/labels\")\n    \n    # Split dataset\n    print(\"Splitting dataset...\")\n    train_count, val_count, test_count = split_dataset(images_path, \"dataset/labels\")\n    \n    # Apply augmentations to training set\n    if apply_augmentation:\n        print(\"Applying augmentations to training set...\")\n        aug_count = apply_augmentations(\n            \"dataset/images/train\", \n            \"dataset/labels/train\",\n            \"dataset/images/train_aug\", \n            \"dataset/labels/train_aug\",\n            num_augmentations=3\n        )\n        \n        # Merge original and augmented data\n        for file_type in [\"images\", \"labels\"]:\n            aug_dir = f\"dataset/{file_type}/train_aug\"\n            train_dir = f\"dataset/{file_type}/train\"\n            \n            if os.path.exists(aug_dir):\n                for file in os.listdir(aug_dir):\n                    shutil.copy(os.path.join(aug_dir, file), os.path.join(train_dir, file))\n                \n                # Remove temporary augmentation directory\n                shutil.rmtree(aug_dir)\n        \n        print(f\"Final training set: {train_count + aug_count} images (including {aug_count} augmented)\")\n    \n    # Create dataset.yaml for YOLOv8\n    yaml_content = f\"\"\"\n    path: ./dataset\n    train: images/train\n    val: images/val\n    test: images/test\n    \n    nc: {len(classes)}\n    names: {classes}\n    \"\"\"\n    \n    with open(\"dataset.yaml\", \"w\") as f:\n        f.write(yaml_content)\n    \n    print(\"Dataset preparation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:26:16.177775Z","iopub.execute_input":"2025-03-11T13:26:16.178094Z","iopub.status.idle":"2025-03-11T13:26:16.195810Z","shell.execute_reply.started":"2025-03-11T13:26:16.178062Z","shell.execute_reply":"2025-03-11T13:26:16.194983Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ============== MODEL TRAINING ==============\n\ndef train_model(epochs=50, img_size=640, batch_size=16):\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import DataLoader\n    from ultralytics import YOLO\n    from ultralytics.data.dataset import YOLODataset\n    from ultralytics.utils.metrics import box_iou\n    from tqdm import tqdm\n    import numpy as np\n    import time\n    import yaml\n\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n\n    # Load YOLOv8m model\n    model = YOLO('yolov8m.pt')\n\n    # Multi-GPU support\n    if torch.cuda.device_count() > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Get the model's task-specific modules\n    # model = model.model.to(device)\n\n    # Create datasets and dataloaders\n    train_dataset = YOLODataset(\n        img_path=\"dataset/images/train\",\n        data={\"names\": classes},\n        imgsz=img_size,\n        augment=True,\n        # hyp={\"flipud\": 0.0, \"fliplr\": 0.5, \"mixup\": 0.0},\n        prefix=\"train\"\n    )\n    \n    val_dataset = YOLODataset(\n        img_path=\"dataset/images/val\",\n        data={\"names\": classes},\n        imgsz=img_size,\n        augment=False,\n        # hyp=None,\n        prefix=\"val\"\n    )\n    \n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size * torch.cuda.device_count(),  # Scale batch size\n        shuffle=True,\n        num_workers=4,\n        collate_fn=train_dataset.collate_fn,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size * torch.cuda.device_count(),\n        shuffle=False,\n        num_workers=4,\n        collate_fn=val_dataset.collate_fn,\n        pin_memory=True\n    )\n    \n    # Optimizer, scheduler, and loss function\n    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.937, weight_decay=0.0005)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    # compute_loss = model.module.loss if hasattr(model, 'module') else model.loss\n\n\n    # Training loop\n    best_map = 0\n    start_time = time.time()\n\n    model.train(data='dataset/data.yaml', epochs=epochs, imgsz=img_size, batch=batch_size)\n\n    # Training complete\n    total_time = time.time() - start_time\n    print(f\"Training complete in {total_time/60:.2f} minutes\")\n    print(f\"Best mAP: {best_map:.4f}\")\n    \n    # Load best model\n    model.load_state_dict(torch.load(\"best_model.pt\"))\n\n    # Convert back to YOLO format for saving\n    yolo_model = YOLO('yolov8m.pt')\n    yolo_model.model = model\n\n    return yolo_model, {\"best_map\": best_map}, {\"training_time\": total_time}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:35:45.802049Z","iopub.execute_input":"2025-03-11T13:35:45.802433Z","iopub.status.idle":"2025-03-11T13:35:45.819464Z","shell.execute_reply.started":"2025-03-11T13:35:45.802407Z","shell.execute_reply":"2025-03-11T13:35:45.818265Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Install required packages\nimport subprocess\nsubprocess.run([\"pip\", \"install\", \"ultralytics\", \"albumentations\"])\n    \n# Paths for dataset directories\nannotations_path = \"/kaggle/input/pulmonary-nodule-detection/train/anno\"\nimages_path = \"/kaggle/input/pulmonary-nodule-detection/train/jpg\"\n    \n# Check if directories exist\nif not os.path.isdir(annotations_path) or not os.path.isdir(images_path):\n    print(f\"Warning: One or more directories don't exist: {annotations_path}, {images_path}\")\n    print(\"Please update the paths to match your dataset location.\")\n    \n# Prepare dataset\nprepare_dataset(annotations_path, images_path, apply_augmentation=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:26:16.217495Z","iopub.execute_input":"2025-03-11T13:26:16.217763Z","iopub.status.idle":"2025-03-11T13:29:27.748910Z","shell.execute_reply.started":"2025-03-11T13:26:16.217728Z","shell.execute_reply":"2025-03-11T13:29:27.747474Z"}},"outputs":[{"name":"stdout","text":"Converting annotations to YOLO format...\nSplitting dataset...\nDataset split complete: 1050 training, 225 validation, 225 test images\nApplying augmentations to training set...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n<ipython-input-6-9f5f80880665>:28: UserWarning: Argument 'alpha_affine' is not valid and will be ignored.\n  A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.5),\n<ipython-input-6-9f5f80880665>:30: DeprecationWarning: Flip is deprecated. Consider using HorizontalFlip, VerticalFlip, RandomRotate90 or D4.\n  A.Flip(p=0.5),\n","output_type":"stream"},{"name":"stdout","text":"Created 3150 augmented images\nFinal training set: 4200 images (including 3150 augmented)\nDataset preparation complete!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Train model\nmodel, results, metrics = train_model(epochs=50)\n    \n# Save model\nmodel.save(\"yolov8m_pulmonary.pt\")\n    \nprint(\"Training complete! Model saved as 'yolov8m_pulmonary.pt'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:35:52.365733Z","iopub.execute_input":"2025-03-11T13:35:52.366077Z","iopub.status.idle":"2025-03-11T13:35:52.710937Z","shell.execute_reply.started":"2025-03-11T13:35:52.366050Z","shell.execute_reply":"2025-03-11T13:35:52.709741Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"trainScanning dataset/labels/train.cache... 4200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 4200/4200 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\nvalScanning dataset/labels/val.cache... 225 images, 0 backgrounds, 0 corrupt: 100%|██████████| 225/225 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-45db73b0868e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"yolov8m_pulmonary.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-278e433bf785>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, img_size, batch_size)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dataset/data.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Training complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Module.train() got an unexpected keyword argument 'data'"],"ename":"TypeError","evalue":"Module.train() got an unexpected keyword argument 'data'","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}